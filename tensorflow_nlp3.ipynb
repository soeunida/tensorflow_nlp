{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## tensorflow_nlp.ipynb\r\n",
    "## NLP : 자연어 처리\r\n",
    "## 1) Classification (분류) - 라벨링이 필요\r\n",
    "## 2) Machine Translation (번역)\r\n",
    "## 3) Text Generation (문장 생성)\r\n",
    "## 4) Voice Assistants (보이스 비서)\r\n",
    "\r\n",
    "==> 이런 자연어처리들을 하기 위해서 필요한 것이 -> sequence prolems!!! (문장을 이루는 단어를 숫자로 표현하는 문제)\r\n",
    "==> NLP (Natural Language Processing) : 자연어에서 정보를 추려해는 것인 목적\r\n",
    "==> NLU (Natural Language Understanding) : 자연어를 이해하고 행동이 목적\r\n",
    "\r\n",
    "자연어에서 이야기하는 용어\r\n",
    "1) Text \r\n",
    "2) Speech "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 우리가 하려는 과정은\r\n",
    "텍스트 -> 이 텍스트를 숫자로 변환 -> 모델 생성 -> 패턴을 찾기 위해 모델을 학습 -> 패턴 이용 (예측)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "unzip_data(\"nlp_getting_started.zip\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "train_df = pd.read_csv(\"train.csv\")\r\n",
    "test_df = pd.read_csv(\"test.csv\")\r\n",
    "\r\n",
    "train_df.head(), train_df.shape, test_df.head(), test_df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       " 1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       " 2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       " 3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       " 4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " (7613, 5),\n",
       "    id keyword location                                               text\n",
       " 0   0     NaN      NaN                 Just happened a terrible car crash\n",
       " 1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       " 2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       " 3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       " 4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan,\n",
       " (3263, 4))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_df_shuffled = train_df.sample(frac = 1, random_state=42)\r\n",
    "train_df_shuffled.head(), train_df_shuffled.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(        id      keyword               location  \\\n",
       " 2644  3796  destruction                    NaN   \n",
       " 2227  3185       deluge                    NaN   \n",
       " 5448  7769       police                     UK   \n",
       " 132    191   aftershock                    NaN   \n",
       " 6845  9810       trauma  Montgomery County, MD   \n",
       " \n",
       "                                                    text  target  \n",
       " 2644  So you have a new weapon that can cause un-ima...       1  \n",
       " 2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       " 5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       " 132   Aftershock back to school kick off was great. ...       0  \n",
       " 6845  in response to trauma Children of Addicts deve...       0  ,\n",
       " (7613, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 우리가 하려는 것\r\n",
    "## 입력 (text 컬럼) -> 머신러닝 알고리즘 -> 출력 (target 컬럼)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_df.target.value_counts()\r\n",
    "\r\n",
    "# 결과값이 0과 1밖에 없기에 binary classification 문제"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(f\"총 학습할 데이터의 수 : {len(train_df)}\")\r\n",
    "print(f\"총 테스트할 데이터의 수 : {len(test_df)}\")\r\n",
    "print(f\"총 데이터의 수 : {len(train_df) + len(test_df)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "총 학습할 데이터의 수 : 7613\n",
      "총 테스트할 데이터의 수 : 3263\n",
      "총 데이터의 수 : 10876\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import random\r\n",
    "\r\n",
    "random_index = random.randint(0, len(train_df) - 5)\r\n",
    "for row in train_df_shuffled[ [\"text\", \"target\"] ][random_index:random_index + 5].itertuples():\r\n",
    "    _, text, target = row\r\n",
    "    print(f\"Target: {target}\", \"부정\" if target > 0 else \"긍정\")\r\n",
    "    print(f\"Text: \\n{text}\\n\")\r\n",
    "    print(\"---\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target: 1 부정\n",
      "Text: \n",
      "ÛÏRichmond Coaches were devastated to hear of the death of their second driver Mr Chance who was sittingÛ_: Jam... http://t.co/sHKiMonMlw\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 긍정\n",
      "Text: \n",
      "im getting a car wow it hasn't sunk in\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 긍정\n",
      "Text: \n",
      "@ViralSpell: 'Couple spend wedding day feeding 4000 Syrian refugees. http://t.Û_ http://t.co/I1VPkQ9yAg see more http://t.co/tY5GAvn7uk\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 부정\n",
      "Text: \n",
      "70th anniversary of Hiroshima atomic bombing marked  http://t.co/1mGvd4x5Oe\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 부정\n",
      "Text: \n",
      "Grace: here are damage levels USA style.. #Taiwan #China #world hurricane/typhoon ratings/categories defined again http://t.co/OdYdT9QPk1\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "type(train_df_shuffled[\"text\"]), train_df_shuffled[\"text\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(pandas.core.series.Series,\n",
       " 2644    So you have a new weapon that can cause un-ima...\n",
       " 2227    The f$&amp;@ing things I do for #GISHWHES Just...\n",
       " 5448    DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...\n",
       " 132     Aftershock back to school kick off was great. ...\n",
       " 6845    in response to trauma Children of Addicts deve...\n",
       "                               ...                        \n",
       " 5226    @Eganator2000 There aren't many Obliteration s...\n",
       " 5390    just had a panic attack bc I don't have enough...\n",
       " 860     Omron HEM-712C Automatic Blood Pressure Monito...\n",
       " 7603    Officials say a quarantine is in place at an A...\n",
       " 7270    I moved to England five years ago today. What ...\n",
       " Name: text, Length: 7613, dtype: object)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "type(train_df_shuffled[\"text\"].to_numpy()), train_df_shuffled[\"text\"].to_numpy()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array(['So you have a new weapon that can cause un-imaginable destruction.',\n",
       "        'The f$&amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons. Thx @mishacollins @/@',\n",
       "        'DT @georgegalloway: RT @Galloway4Mayor: \\x89ÛÏThe CoL police can catch a pickpocket in Liverpool Stree... http://t.co/vXIn1gOq4Q',\n",
       "        ...,\n",
       "        'Omron HEM-712C Automatic Blood Pressure Monitor STANDARD AND LARGE BP CUFFS http://t.co/gJBAInQWN9 http://t.co/jPhgpL1c5x',\n",
       "        'Officials say a quarantine is in place at an Alabama home over a possible Ebola case after developing symptoms... http://t.co/rqKK15uhEY',\n",
       "        'I moved to England five years ago today. What a whirlwind of time it has been! http://t.co/eaSlGeA1B7'],\n",
       "       dtype=object))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\r\n",
    "    train_df_shuffled[\"text\"].to_numpy(),\r\n",
    "    train_df_shuffled[\"target\"].to_numpy(),\r\n",
    "    test_size = 0.1,\r\n",
    "    random_state= 42\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_sentences[:10], train_labels[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이제 모든 데이터 준비가 끝났습니다!\r\n",
    "## 라벨인 target은 숫자! -> 기계가 아주 잘 인식!!!\r\n",
    "## 문제는 text가 문자열! -> 기계가 인식하기 어려움!!!\r\n",
    "## 문자열을 숫자로 바꾸어 주는 과정이 필요합니다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLP에서는 텍스트를 숫자로 바꾸어주는 2가지 주요 개념이 존재!\r\n",
    "## 1) Tokenization : 단어, 문자 등을 바로 숫자로 변환\r\n",
    "       1. Word-level tokenization : 모든 기본 단위가 word\r\n",
    "       2. Chacracter-level tokenization : A ~ Z => 1 ~ 26, 모든 글자가 단위\r\n",
    "       3. Sub-word-level tokenization : word-level tokenization과 character-level tokenization의 중간 / favorite => favor + rite\r\n",
    "## 2) Embedding : 자연어를 학습할 수 있는 형태로 표현한 것, 이런 표현은 feature vector의 형태로 제공합니다.\r\n",
    "      dance라는 단어가 있을 때 [-0.3412, 0.3424, -0.2343, 0.9934, 0.1112]와 같이 표현됨\r\n",
    "      feature vector의 크기는 조정이 가능하다.\r\n",
    "      1. 자신만의 embedding을 사용\r\n",
    "      2. 사전에 학습된 embedding을 사용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 각각 어떤 것을 사용해야하는 것은 문제에 따라 다르다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # Tensorflow 2.6 이전 방식"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\r\n",
    "    max_tokens = None,\r\n",
    "    standardize = \"lower_and_strip_punctuation\",\r\n",
    "    split = \"whitespace\",\r\n",
    "    ngrams = None,\r\n",
    "    output_mode = \"int\",\r\n",
    "    output_sequence_length = None\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "round(sum(\r\n",
    "    [len(i.split()) for i in train_sentences]) / len(train_sentences)\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "max_vocab_length = 10000\r\n",
    "max_length = 15\r\n",
    "\r\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\r\n",
    "    max_tokens = max_vocab_length,\r\n",
    "    standardize = \"lower_and_strip_punctuation\",\r\n",
    "    split = \"whitespace\",\r\n",
    "    ngrams = None,\r\n",
    "    output_mode = \"int\",\r\n",
    "    output_sequence_length = max_length\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "sample_sentence = \"There's a flood in my street!\"\r\n",
    "text_vectorizer([sample_sentence])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ndims'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8944/2448844681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"There's a flood in my street!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\20210906\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autocast\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210906\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210906\\venv\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;31m# the expression needs to evaluate to True in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=g-comparison-negation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         raise RuntimeError(\n\u001b[0;32m    417\u001b[0m             \u001b[1;34m\"When using TextVectorization to tokenize strings, the innermost \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndims'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "sample_sentence = \"There's a flood in my street!\"\r\n",
    "text_vectorizer([sample_sentence])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "sample_sentence = \"I love tensorflow\"\r\n",
    "text_vectorizer([sample_sentence])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  8, 110,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "sample_sentence = \"I Can do TensorFlow\"\r\n",
    "text_vectorizer([sample_sentence])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 8, 71, 68,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "random_sentence = random.choice(train_sentences)\r\n",
    "print(f\"원래 Text : \\n{random_sentence}\\n\\nVector : \")\r\n",
    "text_vectorizer([random_sentence])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원래 Text : \n",
      "Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding\n",
      "\n",
      "Vector : \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 847,  593, 2662,    9,  209, 2995, 5521,  637,    3,  642,    8,\n",
       "         199,    4,  593, 2662]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "type(words_in_vocab), len(words_in_vocab)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 10000)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "top_5_words = words_in_vocab[:5]  # [UNK] : unknown 를 의미\r\n",
    "bottom_5_words = words_in_vocab[-5:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "print(f\"vocab에 있는 단어 총 갯수 : {len(words_in_vocab)}\")\r\n",
    "print(f\"자주 사용하는 단어 5개 : {top_5_words}\")\r\n",
    "print(f\"자주 사용하지 않는 단어 5개 : {bottom_5_words}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab에 있는 단어 총 갯수 : 10000\n",
      "자주 사용하는 단어 5개 : ['', '[UNK]', 'the', 'a', 'in']\n",
      "자주 사용하지 않는 단어 5개 : ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 정말 간단하게 tokenization을 했습니다. 문장을 숫자로 이루어진 벡터로 만들었습니다.\r\n",
    "## embedding의 특징 : tokennization의 경우 tensorflow = 1, I = 8\r\n",
    "##                   이 값들은 우리가 다시 새로운 데이터셋으로 adopt를 하지 않으면\r\n",
    "##                   계속 유지\r\n",
    "## embedding은 단어들간의 관계 (relation)를 표현하기 때문에, 학습을 하면서 변경이 된다! 즉 개선이 된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "embedding = layers.Embedding(\r\n",
    "    input_dim = max_vocab_length,\r\n",
    "    output_dim = 128, # embedding vector의 크기\r\n",
    "    embeddings_initializer = \"uniform\",\r\n",
    "    input_length = max_length\r\n",
    ")\r\n",
    "embedding"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x232bc8c4df0>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "random_sentence = random.choice(train_sentences)\r\n",
    "print(f\"원래 Text : \\n{random_sentence}\\n\\nEmbedded : \")\r\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\r\n",
    "sample_embed"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원래 Text : \n",
      "I LAVA YOU.\n",
      "\n",
      "Embedded : \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.03842573,  0.01449363, -0.01360431, ..., -0.0302658 ,\n",
       "          0.02257574, -0.02653332],\n",
       "        [ 0.01334765, -0.01756917, -0.01153872, ..., -0.0064621 ,\n",
       "         -0.0351951 , -0.04888349],\n",
       "        [ 0.04480051, -0.0264637 ,  0.02962433, ..., -0.02560654,\n",
       "          0.01879858,  0.01700368],\n",
       "        ...,\n",
       "        [-0.04715964,  0.03798088,  0.01226502, ...,  0.01601822,\n",
       "          0.04542531,  0.0255288 ],\n",
       "        [-0.04715964,  0.03798088,  0.01226502, ...,  0.01601822,\n",
       "          0.04542531,  0.0255288 ],\n",
       "        [-0.04715964,  0.03798088,  0.01226502, ...,  0.01601822,\n",
       "          0.04542531,  0.0255288 ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "sample_embed[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-0.03842573,  0.01449363, -0.01360431, -0.04006778,  0.00339891,\n",
       "       -0.04589501,  0.01881088, -0.00758136,  0.04664798, -0.013203  ,\n",
       "       -0.00238113, -0.04470152, -0.01492572,  0.01903288, -0.02764055,\n",
       "        0.03634011, -0.00748052, -0.03234205, -0.02379128,  0.02365288,\n",
       "        0.01844326, -0.04424445, -0.00243623, -0.02265985,  0.0157547 ,\n",
       "       -0.01304866, -0.02187171, -0.03622397,  0.02546528,  0.03338465,\n",
       "        0.01150749,  0.00638796, -0.00525988, -0.01641666, -0.04038722,\n",
       "        0.02887448,  0.02615959, -0.03832792,  0.0294642 , -0.0489084 ,\n",
       "        0.02392549,  0.03432212,  0.00364852, -0.03967185,  0.00497659,\n",
       "        0.01708763,  0.04872471,  0.02718883, -0.00628611, -0.04688453,\n",
       "       -0.00987806,  0.04842838,  0.04946474, -0.00306113, -0.0237103 ,\n",
       "        0.03145838, -0.02748504, -0.00488142, -0.0008931 ,  0.03056872,\n",
       "        0.04024469,  0.01253624, -0.01311201,  0.02665125,  0.02681747,\n",
       "       -0.03092592, -0.04728309, -0.03200294, -0.03571185,  0.03654561,\n",
       "        0.03710593, -0.01166682, -0.04308193,  0.04470804, -0.02097527,\n",
       "        0.03542194,  0.03863979,  0.01539561,  0.0457756 , -0.01904743,\n",
       "       -0.00590276, -0.00185394, -0.01712499,  0.00321976, -0.02295905,\n",
       "        0.0328987 ,  0.03884983,  0.01463774,  0.04346294, -0.02510113,\n",
       "       -0.04398979, -0.00769015,  0.01992709,  0.04204906, -0.03319652,\n",
       "       -0.02975459,  0.01219567,  0.04661966,  0.00185187,  0.04787279,\n",
       "        0.01711838,  0.0283266 ,  0.02354434, -0.04574167, -0.02489163,\n",
       "        0.02131918,  0.02480871, -0.02582755, -0.01705139, -0.02426072,\n",
       "       -0.03060037, -0.00170197, -0.02784307,  0.01712069,  0.02111695,\n",
       "        0.00375819, -0.00660647,  0.03313747, -0.04871298, -0.04466705,\n",
       "       -0.04318747, -0.03177762,  0.00251352,  0.03631162, -0.0007054 ,\n",
       "       -0.0302658 ,  0.02257574, -0.02653332], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이제 우리는 학습할 수 있는 데이터를 준비했습니다.\r\n",
    "## 1. Model 0 : Naive Bayes\r\n",
    "## 2. Model 1 : Feed-forward neural network (dense model)\r\n",
    "## 3. Model 2 : LSTM\r\n",
    "## 4. Model 3 : GRU\r\n",
    "## 5. Model 4 : Bidirectional-LSTM\r\n",
    "## 6. Model 5 : 1D CNN\r\n",
    "## 7. Model 6 : Tensorflow Hub pretrained feature extractor \r\n",
    "## 8. Model 7 : 전이학습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model_0 = Pipeline([\r\n",
    "    (\"tfidf\", TfidfVectorizer()), # tfidf를 사용해서 단어를 숫자로 변경\r\n",
    "    (\"clf\", MultinomialNB()) # 텍스트를 모델링\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "model_0.fit(train_sentences, train_labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\r\n",
    "print(f\"Model 0의 accuracy는 {baseline_score * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model 0의 accuracy는 79.27%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "baseline_preds = model_0.predict(val_sentences)\r\n",
    "baseline_preds[:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def calculate_results(y_true, y_pred):\r\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\r\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\r\n",
    "    model_results = {\r\n",
    "        \"accuracy\" : model_accuracy,\r\n",
    "        \"precision\" : model_precision,\r\n",
    "        \"recall\" : model_recall,\r\n",
    "        \"f1\" : model_f1\r\n",
    "    }\r\n",
    "    return model_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "baseline_results = calculate_results(y_true = val_labels, y_pred = baseline_preds)\r\n",
    "baseline_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f17e0dbfb3d668e70ba8dc788ddd7948a9e194538bbaddbd20e0f53ce1afb686"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}